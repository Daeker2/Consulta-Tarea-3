# -*- coding: utf-8 -*-
"""Tarea 3 NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wJjoo7un5zfmK4-p7Z7r_d3X-4bmrrAh

#Librerias
"""

# --- INSTALACIÓN DE LIBRERÍAS NECESARIAS ---

# pdfplumber: extraer texto de PDFs
# spacy: procesamiento de lenguaje natural
# flair: reconocimiento de entidades nombradas (NER)
# openpyxl: guardar resultados en Excel
# nltk: tokenización y stopwords
# stanza: tokenización y anotaciones
!pip install pdfplumber spacy flair openpyxl stanza
!python -m spacy download es_core_news_sm
!python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"

# --- IMPORTACIÓN DE LIBRERÍAS ---
import pdfplumber           # Para leer PDFs
import re                   # Expresiones regulares
import unicodedata          # Normalización de caracteres Unicode
import spacy                # Procesamiento de lenguaje natural
import nltk
import stanza
from transformers import AutoTokenizer
from textblob import TextBlob
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from flair.data import Sentence
from flair.models import SequenceTagger
import pandas as pd         # Para manejar DataFrames
from collections import Counter

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('punkt_tab')
stanza.download('es')

"""#Resto del código"""

# --- CARGA DE MODELOS ---
# Modelo de spaCy para lematización
nlp = spacy.load("es_core_news_sm")

# Modelo de Flair para NER en español
tagger = SequenceTagger.load('flair/ner-spanish-large')

nlp_stanza = stanza.Pipeline('es')  # Stanza para tokenización y anotaciones
tokenizer_bert = AutoTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")  # BERT

# --- RUTA DEL ARCHIVO PDF ---
pdf_path = "/content/Cronica.pdf"

# --- FUNCIÓN PARA EXTRAER TEXTO DEL PDF ---
def extract_text(pdf_path):
    """
    Extrae todo el texto de un PDF y lo devuelve como un solo string.
    """
    texto = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text_page = page.extract_text()
            if text_page:
                texto += text_page + " "  # Unir páginas con espacio
    return texto

# --- FUNCIÓN DE PREPROCESAMIENTO DE TEXTO ---
def preprocess_text(text):
    """
    Limpia y prepara el texto:
    1. Normalización Unicode
    2. Eliminación de caracteres no deseados y números
    3. Conversión a minúsculas
    4. Tokenización, lematización y eliminación de stop words
    """
    # 1. Normalización UNICODE
    text = unicodedata.normalize("NFKD", text)

    # 2. Eliminar caracteres especiales
    text = re.sub(r'[^\w\s]', '', text)

    # 3. Convertir a minúsculas
    text = text.lower()

    # 4. Tokenización y lematización
    stop_words = set(stopwords.words('spanish'))
    # Quitar palabras de negación de los stopwords
    negaciones = {"no", "nunca", "nadie", "ninguno", "sin"}
    stop_words = stop_words - negaciones
    tokens = [token.lemma_ for token in nlp(text)
              if token.is_alpha and token.text not in stop_words]

    return tokens

# --- FUNCIÓN PARA RECONOCIMIENTO DE ENTIDADES NOMBRADAS (NER) ---
def extract_ner(text):
    """
    Aplica Flair NER al texto y devuelve una lista de tuplas:
    (texto_entidad, tipo_entidad)
    """
    sentence = Sentence(text)
    tagger.predict(sentence)
    entities = [(entity.text, entity.labels[0].value) for entity in sentence.get_spans('ner')]
    return entities

# --- FUNCIÓN PARA CONTAR FRECUENCIA DE ELEMENTOS ---
def count_frequency(items):
    """
    Recibe una lista y devuelve un DataFrame con los elementos y su frecuencia,
    ordenado de mayor a menor frecuencia.
    """
    counter = Counter(items)
    return pd.DataFrame(counter.items(), columns=["Item", "Frecuencia"]).sort_values(by="Frecuencia", ascending=False)

# --- PIPELINE COMPLETO ---
# 1. Extraer texto del PDF
raw_text = extract_text(pdf_path)

# 2. Preprocesar el texto (Unicode, limpieza, minúsculas, tokenización, lematización, stop words)
tokens = preprocess_text(raw_text)

# 3. Guardar tokens preprocesados en TXT
with open("/content/cronica_limpia.txt", "w", encoding="utf-8") as f:
    f.write(" ".join(tokens))

# 4. Tokenización con múltiples métodos
print("\n--- Tokenización con múltiples métodos ---")

# 4a. Tokenización con NLTK
tokens_nltk = word_tokenize(" ".join(tokens), language='spanish')
print("Primeros 20 tokens con NLTK:", tokens_nltk[:20])

# 4b. Tokenización con Stanza
doc_stanza = nlp_stanza(" ".join(tokens))
tokens_stanza = [word.text for sent in doc_stanza.sentences for word in sent.words]
print("Primeros 20 tokens con Stanza:", tokens_stanza[:20])

# 4c. Tokenización con BERT
tokens_bert = tokenizer_bert.tokenize(" ".join(tokens))
print("Primeros 20 tokens con BERT:", tokens_bert[:20])

# 4d. Tokenización con TextBlob
blob = TextBlob(" ".join(tokens))
tokens_textblob = blob.words
print("Primeros 20 tokens con TextBlob:", tokens_textblob[:20])

# 5. Reconocimiento de entidades (NER)
text_for_ner = " ".join(tokens)
entities = extract_ner(text_for_ner)

# 6. Guardar entidades en Excel con su frecuencia
df_entities = count_frequency([f"{e[0]} ({e[1]})" for e in entities])
df_entities.to_excel("/content/cronica_ner.xlsx", index=False)

print("Tokens preprocesados y NER guardados exitosamente.")