# -*- coding: utf-8 -*-
"""Nestort Taller_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IY74EU3M7Kv_9dtDcwb98bxdWe2WJm0K
"""

# Instalamos las librerías necesarias (si no están instaladas)
!pip install PyMuPDF spacy nltk stanza transformers

!python -m spacy download es_core_news_sm

import fitz  # PyMuPDF para leer PDF
import unicodedata
import spacy
import nltk
import stanza
from transformers import AutoTokenizer
import re
import textblob

# Descargar modelos necesarios
nltk.download('punkt')
stanza.download('es')
nlp_stanza = stanza.Pipeline('es')
nlp_spacy = spacy.load('es_core_news_sm')
tokenizer_bert = AutoTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")

# 1. Leer el archivo PDF
def leer_pdf(ruta):
    with fitz.open(ruta) as doc:
        texto = "\n".join([page.get_text("text") for page in doc])
    return texto

ruta_pdf = "C:/Users/ACER/OneDrive/Documentos/Analitica de datos/1. Maestria/7. NLP/Clase_3/ejemplos de ensayos sobre tecnología.pdf"
texto = leer_pdf(ruta_pdf)
print("Texto original:\n", texto[:500])

# 2. Convertir a minúsculas
texto_minusculas = texto.lower()
print("\nTexto en minúsculas:\n", texto_minusculas[:500])

# 3. Normalización Unicode (NFC)
def normalizar_unicode(texto):
    return unicodedata.normalize('NFC', texto)

texto_normalizado = normalizar_unicode(texto_minusculas)
print("\nTexto normalizado:\n", texto_normalizado[:500])

# Download the necessary data for Spanish tokenization
nltk.download('punkt')
nltk.download('spanish_grammars')

# 4. Tokenización con diferentes librerías

def tokenizar_spacy(texto):
    return [token.text for token in nlp_spacy(texto)]

def tokenizar_nltk(texto):
    return nltk.word_tokenize(texto, language='spanish')

def tokenizar_stanza(texto):
    doc = nlp_stanza(texto)
    return [word.text for sent in doc.sentences for word in sent.words]

def tokenizar_bert(texto):
    return tokenizer_bert.tokenize(texto)

# Aplicamos cada método
tokens_spacy = tokenizar_spacy(texto_normalizado)
tokens_nltk = tokenizar_nltk(texto_normalizado)
tokens_stanza = tokenizar_stanza(texto_normalizado)
tokens_bert = tokenizar_bert(texto_normalizado)

nltk.download('punkt_tab')

# Mostramos los primeros 20 tokens de cada método
print("\nTokens con SpaCy:", tokens_spacy[:20])
print("\nTokens con NLTK:", tokens_nltk[:20])
print("\nTokens con Stanza:", tokens_stanza[:20])
print("\nTokens con BERT:", tokens_bert[:20])

tokens_bert = tokenizer_bert.tokenize(texto_normalizado)

# Dividimos en fragmentos de 512 tokens
max_length = 512
chunks = [tokens_bert[i:i+max_length] for i in range(0, len(tokens_bert), max_length)]

print(f"Se generaron {len(chunks)} fragmentos de 512 tokens")

def tokenizar_bert(texto):
    return tokenizer_bert.tokenize(texto)

print("\nTokens con BERT:", tokens_bert[:30])

# Eliminar stop words usando SpaCy
def eliminar_stopwords(tokens, nlp):
    return [token for token in tokens if token not in nlp.Defaults.stop_words]

tokens_sin_stopwords = eliminar_stopwords(tokens_spacy, nlp_spacy)
print("\nTokens sin stop words (primeros 20):", tokens_sin_stopwords[:20])

import re

# Buscar números en el texto
numeros = re.findall(r'\d+', texto_minusculas)
print("Números encontrados en el texto:", numeros)

for idx, numero in enumerate(numeros, start=1):
    print(f"Número {idx}: {numero}")

# Importamos la librería 're', que es el módulo de expresiones regulares de Python.
# No necesita instalación porque viene incluida con el lenguaje.
import re

# Definimos la función que realizará la tokenización.
# Recibe una cadena de texto como entrada.
def tokenizar_con_regex_y_numeros(texto_minusculas):
    """
    Esta función tokeniza un texto utilizando una expresión regular
    diseñada para separar palabras, números y signos de puntuación.
    """

    # Definimos el patrón de la expresión regular. Se lee como una serie de "O" lógicos (|).
    # r'' indica que es una "raw string" para que los caracteres como '\' no se interpreten como secuencias de escape.
    # 1. \d+      : Busca una o más ocurrencias consecutivas de dígitos (números). Esto captura '500', '2025', '10'.
    # 2. |         : Actúa como un "O".
    # 3. [A-Za-zÁ-ú]+ : Busca una o más ocurrencias de letras, tanto mayúsculas como minúsculas.
    #                  Incluimos 'Á-ú' para capturar caracteres del español como 'á', 'é', 'í', 'ó', 'ú', 'ñ'.
    # 4. |         : Otro "O".
    # 5. [^\w\s]  : Busca cualquier caracter individual que NO sea (\^) un caracter de palabra (\w, que incluye letras, números y _)
    #                  Y que NO sea (\^) un espacio en blanco (\s). Esto nos permite capturar signos de puntuación como '.', ':', '$'.
    patron = r'\d+|[A-Za-zÁ-ú]+|[^\w\s]'

    # Utilizamos la función re.findall() para encontrar todas las subcadenas en el texto
    # que coincidan con nuestro patrón. Devuelve una lista con todos los hallazgos.
    tokens = re.findall(patron, texto_minusculas)

    # La función devuelve la lista de tokens encontrados.
    return tokens

# --- Proceso de ejecución ---

# 1. Definimos un texto de ejemplo que contiene palabras, números y puntuación.
# texto_ejemplo = "En 2025, el producto A-42 costará $500.50 y la entrega será a las 10:30."

# 2. Llamamos a nuestra función personalizada para tokenizar el texto.
tokens_resultado = tokenizar_con_regex_y_numeros(texto_minusculas)

# 3. Imprimimos el resultado para verificar cómo se separaron los elementos.
print(f"Texto original:\n{texto_minusculas}\n")
print(f"Tokens generados ({len(tokens_resultado)} tokens):\n{tokens_resultado}")

!pip install spacy
!python -m spacy download es_core_news_sm
!pip install pdfplumber --no-cache-dir

import pdfplumber  # Para extraer texto de PDFs
import re  # Para trabajar con expresiones regulares
import spacy  # Para procesamiento de lenguaje natural (NLP)
import unicodedata  # Para normalizar caracteres Unicode

nlp = spacy.load("es_core_news_sm")  #cargar el modelo en español

# Importa la librería fitz, que es el nombre del módulo PyMuPDF
import fitz

# Usa la misma ruta de tu archivo
pdf_path = "C:/Users/ACER/OneDrive/Documentos/Analitica de datos/1. Maestria/7. NLP/Clase_3/ejemplos de ensayos sobre tecnología.pdf"
texto_pdf = ""

try:
    # Abre el documento PDF con fitz
    with fitz.open(pdf_path) as doc:
        # Itera sobre cada página del documento
        for pagina in doc:
            # Extrae el texto de la página y lo añade a la variable
            texto_pdf += pagina.get_text()

    # Imprime el texto extraído (o una parte para verificar)
    print("¡Texto extraído con éxito!")
    print(texto_pdf[:500]) # Imprime los primeros 500 caracteres

except Exception as e:
    # Si ocurre un error, imprímelo para saber qué pasó
    print(f"No se pudo leer el PDF. El archivo podría estar dañado o no ser un PDF válido.")
    print(f"Error original: {e}")

pdf_path = "C:/Users/ACER/OneDrive/Documentos/Analitica de datos/1. Maestria/7. NLP/Clase_3/ejemplos de ensayos sobre tecnología.pdf"
# Extraer texto del PDF
with pdfplumber.open(pdf_path) as pdf:
    texto_pdf = ""
    for pagina in pdf.pages:
        texto_pdf += pagina.extract_text()

header_pattern = r"TECNOLOGIA Y SUS EJEMPLOS 2024"

def extract_text_without_header(pdf_path):
    extracted_text = []  # Lista donde almacenaremos el texto de cada página

    with pdfplumber.open(pdf_path) as pdf:  # Abrir el archivo PDF
        for page in pdf.pages:  # Iterar por cada página del PDF
            text = page.extract_text()  # Extraer texto de la página
            if text:  # Verificar si hay texto en la página
                text = re.sub(header_pattern, "", text, flags=re.DOTALL)  # Eliminar encabezado con regex
                extracted_text.append(text)  # Guardar el texto limpio en la lista

    return " ".join(extracted_text)  # Unir todas las páginas en un solo texto

def preprocess_text(text):
    text = unicodedata.normalize("NFKD", text)  # Normalizar caracteres Unicode (ej. á → a)

    doc = nlp(text.lower())  # Convertir todo el texto a minúsculas y procesarlo con spaCy

    tokens = [token.text for token in doc if token.is_alpha and not token.is_stop]
    return tokens

raw_text = extract_text_without_header(pdf_path)  # Extraer texto limpio del PDF
tokens = preprocess_text(raw_text)  # Preprocesar el texto con spaCy

print(tokens[:50])  # Mostrar los primeros 50 tokens como prueba

with open("TALLER_3.txt", "w", encoding="utf-8") as f:
    f.write(" ".join(tokens))  # Guardar los tokens en un archivo de texto
print("Tokens guardados en 'TALLER_3.txt'")